# 03 Feature Selection
from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import chi2
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from joblib import load

PROJECT_DIR = Path(__file__).resolve().parents[1]
DATA_PATH = PROJECT_DIR / "data" / "heart_disease.csv"
pre = load(PROJECT_DIR / "models" / "preprocessor.pkl")

raw = pd.read_csv(DATA_PATH)
raw.columns = raw.columns.str.lower().str.strip()
if 'heartdisease' in raw.columns and 'target' not in raw.columns:
    raw = raw.rename(columns={'heartdisease': 'target'})
X = raw.drop(columns=['target'])
y = raw['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

Xt = pre["preprocessor"].fit_transform(X_train)

# Chi2 only for non-negative features; ensure scaled positives by MinMax, but we used StandardScaler.
# For demo with placeholder, take absolute to avoid negatives in tiny dataset.
Xt_pos = np.abs(Xt)
chi2_scores, pvals = chi2(Xt_pos, y_train)

# RandomForest feature importance
rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(Xt, y_train)
rf_importances = rf.feature_importances_

# RFE with RF
rfe = RFE(estimator=RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=min(5, Xt.shape[1]))
rfe.fit(Xt, y_train)

# Plot importances
plt.figure(figsize=(8,4))
plt.bar(range(len(rf_importances)), rf_importances)
plt.title('RandomForest Importances (transformed features)')
plt.tight_layout()
plt.savefig(PROJECT_DIR / 'results' / 'feature_importance_rf.png', dpi=150)
plt.show()

print('Top features by RF (indices):', np.argsort(rf_importances)[-10:][::-1])
print('Selected by RFE (mask):', rfe.support_)